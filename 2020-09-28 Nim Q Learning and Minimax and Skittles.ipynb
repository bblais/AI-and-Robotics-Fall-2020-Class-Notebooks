{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  0.2.21\n"
     ]
    }
   ],
   "source": [
    "from Game import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nim\n",
    "\n",
    "Rules:\n",
    "\n",
    "1. start with 21 sticks -- state = number of sticks\n",
    "2. turns alternate taking 1,2, or 3 sticks\n",
    "3. player taking last stick loses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state():\n",
    "    return 21\n",
    "\n",
    "def show_state(state):\n",
    "    print(\"There are \",state,\"sticks.\")\n",
    "    \n",
    "def valid_moves(state,player):\n",
    "    if state==1:\n",
    "        return [1]\n",
    "    elif state==2:\n",
    "        return [1,2]\n",
    "    else:\n",
    "        return [1,2,3]\n",
    "    \n",
    "def update_state(state,player,move):\n",
    "    # move = number of sticks to pick up\n",
    "    new_state=state-move # remove the sticks\n",
    "    return new_state\n",
    "\n",
    "def win_status(state,player):\n",
    "    if state==0:\n",
    "        return 'lose'\n",
    "    elif state==1:\n",
    "        return 'win'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # there is no stalemate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_move(state,player):\n",
    "    print(\"Player \",player)\n",
    "    move=int(input(\"How many sticks?\"))\n",
    "    return move\n",
    "\n",
    "human_agent=Agent(human_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_move(state,player):\n",
    "    possible_moves=valid_moves(state,player)\n",
    "    move=random.choice(possible_moves)\n",
    "    return move\n",
    "random_agent=Agent(random_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Game.minimax import *\n",
    "def minimax_move(state,player):\n",
    "\n",
    "    values,moves=minimax_values(state,player,display=False)\n",
    "    return top_choice(moves,values)\n",
    "\n",
    "\n",
    "minimax_agent=Agent(minimax_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skittles_move(state,player,info):\n",
    "    S=info.S\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "    \n",
    "    \n",
    "    # if Ive never seen this state before\n",
    "    if not state in S:\n",
    "        actions=valid_moves(state,player)\n",
    "\n",
    "        S[state]=Table()\n",
    "        for action in actions:\n",
    "            S[state][action]=3     \n",
    "    \n",
    "    move=weighted_choice(S[state])  # weighted across actions\n",
    "    \n",
    "    # what if there are no skittles for a particular state?\n",
    "    # move is None in that case\n",
    "    \n",
    "    if move is None:\n",
    "        # learn a little bit\n",
    "        if last_state:\n",
    "            S[last_state][last_action]=S[last_state][last_action]-1\n",
    "            if S[last_state][last_action]<0:\n",
    "                S[last_state][last_action]=0\n",
    "        \n",
    "        move=random_move(state,player)\n",
    "    \n",
    "    return move\n",
    "\n",
    "def skittles_after(status,player,info):\n",
    "    S=info.S\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "\n",
    "    if status=='lose':\n",
    "        # learn a little bit\n",
    "        S[last_state][last_action]=S[last_state][last_action]-1\n",
    "        if S[last_state][last_action]<0:\n",
    "            S[last_state][last_action]=0\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "skittles_agent=Agent(skittles_move)\n",
    "skittles_agent.S=Table()\n",
    "skittles_agent.post=skittles_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_move(state,player,info):\n",
    "    Q=info.Q\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "    \n",
    "    α=info.α\n",
    "    γ=info.γ\n",
    "    ϵ=info.ϵ\n",
    "    \n",
    "\n",
    "    # if Ive never seen this state before\n",
    "    if not state in Q:\n",
    "        actions=valid_moves(state,player)\n",
    "\n",
    "        Q[state]=Table()\n",
    "        for action in actions:\n",
    "            Q[state][action]=0     \n",
    "    \n",
    "    # deal with random vs top choice here\n",
    "    if random.random()<ϵ:\n",
    "        move=random_move(state,player)  \n",
    "    else:\n",
    "        move=top_choice(Q[state]) \n",
    "    \n",
    "    # what if there are no skittles for a particular state?\n",
    "    # move is None in that case\n",
    "    \n",
    "    if not last_action is None:  # not the first move\n",
    "        # learn a little bit\n",
    "        # change equation here\n",
    "        reward=0\n",
    "        \n",
    "        # Bellman equation\n",
    "        Q[last_state][last_action] += α*(reward+\n",
    "                         γ*max([Q[state][a] for a in Q[state]])  - \n",
    "                                Q[last_state][last_action])\n",
    "    \n",
    "        \n",
    "    \n",
    "    return move\n",
    "\n",
    "def Q_after(status,player,info):\n",
    "    Q=info.Q\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "\n",
    "    α=info.α\n",
    "    γ=info.γ\n",
    "    ϵ=info.ϵ\n",
    "    \n",
    "    if status=='lose':\n",
    "        reward=-1\n",
    "    elif status=='win':\n",
    "        reward=1\n",
    "    elif status=='stalemate':\n",
    "        reward=0.5\n",
    "    else:\n",
    "        reward=0\n",
    "        \n",
    "    # learn a little bit\n",
    "    Q[last_state][last_action] += α*(reward-Q[last_state][last_action])\n",
    "        \n",
    "\n",
    "\n",
    "Q_agent=Agent(Q_move)\n",
    "Q_agent.Q=LoadTable('Q_data.json')\n",
    "Q_agent.post=Q_after\n",
    "\n",
    "Q_agent.α=0.3  # learning rate\n",
    "Q_agent.γ=0.9  # memory constant, discount factor\n",
    "Q_agent.ϵ=0.1  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning stage\n",
    "\n",
    "- the Q values can change\n",
    "- that the agent takes random moves sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_agent.α=0.3  # learning rate\n",
    "Q_agent.ϵ=0.1  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=Game(100)\n",
    "g.display=False\n",
    "g.run(minimax_agent,Q_agent);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing stage\n",
    "\n",
    "- the Q values **do not change**\n",
    "- that the agent **never takes random moves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_agent.α=0.0  # learning rate\n",
    "Q_agent.ϵ=0.0  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=Game(10)\n",
    "g.display=False\n",
    "g.run(minimax_agent,Q_agent);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of games:  10\n",
      "Winning 0.00 percent\n",
      "Losing 100.00 percent\n",
      "Tie 0.00 percent\n"
     ]
    }
   ],
   "source": [
    "g.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: {1: -0.02755878181911, 2: -0.09137518828627986, 3: 0.6548045161102354},\n",
       " 17: {1: -0.28560093664146513,\n",
       "  2: -0.35209008610162745,\n",
       "  3: -0.36689125718484517},\n",
       " 13: {1: -0.7178693575103386, 2: -0.730255991088701, 3: -0.7566848912211898},\n",
       " 9: {1: -0.8939054764706701, 2: -0.8930037978068894, 3: -0.8923631333713686},\n",
       " 5: {1: -0.9996090178951418, 2: -0.9997263125265993, 3: -0.9996090178951418},\n",
       " 18: {1: 0.6551058991978964, 2: -0.5238516245907714, 3: -0.035665596},\n",
       " 15: {1: -0.20821947942937508, 2: 0.7287188645048186, 3: -0.3809729052891},\n",
       " 14: {1: 0.7284028814967268, 2: -0.5170723835347195, 3: -0.4522928158670315},\n",
       " 11: {1: -0.31808257341480006, 2: 0.8098515993077074, 3: 0},\n",
       " 6: {1: 0.8998871819717115, 2: -0.657, 3: -0.3},\n",
       " 12: {1: -0.5325680449554707, 2: -0.21739672590588902, 3: 0.8098819461637278},\n",
       " 16: {1: -0.24601660382405044, 2: -0.10533437801910903, 3: 0.728500990955309},\n",
       " 19: {1: -0.07306850481125086, 2: 0.6543479904829171, 3: -0.17656477338191176},\n",
       " 7: {1: 0, 2: 0.8999574131741293, 3: -0.657},\n",
       " 10: {1: 0.8098502078973727, 2: -0.57459723852663, 3: -0.43613623661223455},\n",
       " 3: {1: -0.3, 2: 0.9999999632966318, 3: -0.3},\n",
       " 8: {1: -0.7438189051989298, 2: 0, 3: 0.8999793461621668},\n",
       " 4: {1: -0.8319300000000001, 2: -0.8319300000000001, 3: 0.9999774606597093},\n",
       " 2: {1: 0.9999922690062802, 2: -0.657}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveTable(Q_agent.Q,'Q_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q vs Q with progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_agent=Agent(Q_move)\n",
    "Q1_agent.Q=LoadTable('Q1_data.json')\n",
    "Q1_agent.post=Q_after\n",
    "\n",
    "Q1_agent.α=0.3  # learning rate\n",
    "Q1_agent.γ=0.9  # memory constant, discount factor\n",
    "Q1_agent.ϵ=0.1  # probability of a random move during learning\n",
    "\n",
    "Q2_agent=Agent(Q_move)\n",
    "Q2_agent.Q=LoadTable('Q2_data.json')\n",
    "Q2_agent.post=Q_after\n",
    "\n",
    "Q2_agent.α=0.3  # learning rate\n",
    "Q2_agent.γ=0.9  # memory constant, discount factor\n",
    "Q2_agent.ϵ=0.1  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 : 60.0  20 : 0.0  30 : 100.0  40 : 0.0  50 : 100.0  60 : 0.0  70 : 0.0  80 : 0.0  90 : 0.0  100 : 0.0  110 : 0.0  120 : 0.0  130 : 0.0  140 : 0.0  150 : 0.0  160 : 0.0  170 : 0.0  180 : 0.0  190 : 0.0  200 : 0.0  210 : 0.0  220 : 0.0  230 : 0.0  240 : 0.0  250 : 0.0  260 : 0.0  270 : 0.0  280 : 0.0  290 : 0.0  300 : 0.0  310 : 0.0  320 : 0.0  330 : 0.0  340 : 0.0  350 : 0.0  360 : 0.0  370 : 0.0  380 : 0.0  390 : 0.0  400 : 0.0  410 : 0.0  420 : 0.0  430 : 0.0  440 : 0.0  450 : 0.0  460 : 0.0  470 : 0.0  480 : 0.0  490 : 0.0  500 : 0.0  510 : 0.0  520 : 0.0  530 : 0.0  540 : 0.0  550 : 0.0  560 : 0.0  570 : 0.0  580 : 0.0  590 : 0.0  600 : 0.0  610 : 0.0  620 : 0.0  630 : 0.0  640 : 0.0  650 : 0.0  660 : 0.0  670 : 0.0  680 : 0.0  690 : 0.0  700 : 0.0  710 : 0.0  720 : 0.0  730 : 0.0  740 : 0.0  750 : 0.0  760 : 0.0  770 : 0.0  780 : 0.0  790 : 0.0  800 : 0.0  810 : 0.0  820 : 0.0  830 : 0.0  840 : 0.0  850 : 0.0  860 : 0.0  870 : 0.0  880 : 0.0  890 : 0.0  900 : 0.0  910 : 0.0  920 : 0.0  930 : 0.0  940 : 0.0  950 : 0.0  960 : 0.0  970 : 0.0  980 : 0.0  990 : 0.0  1000 : 0.0  "
     ]
    }
   ],
   "source": [
    "total_number_of_games=0\n",
    "for epoch in range(100):\n",
    "    \n",
    "    number_training_games=10\n",
    "    number_of_testing_games=10\n",
    "    \n",
    "    #=================\n",
    "    # traning cycle\n",
    "    Q1_agent.α=0.3  # learning rate\n",
    "    Q1_agent.ϵ=0.1  # probability of a random move during learning\n",
    "    Q2_agent.α=0.3  # learning rate\n",
    "    Q2_agent.ϵ=0.1  # probability of a random move during learning\n",
    "    \n",
    "    g=Game(number_training_games)\n",
    "    g.display=False\n",
    "    g.run(Q1_agent,Q2_agent)\n",
    "\n",
    "    #=================\n",
    "    # testing cycle\n",
    "    Q1_agent.α=0.0  # learning rate\n",
    "    Q1_agent.ϵ=0.0  # probability of a random move during learning\n",
    "    Q2_agent.α=0.0  # learning rate\n",
    "    Q2_agent.ϵ=0.0  # probability of a random move during learning\n",
    "    \n",
    "    \n",
    "    g=Game(number_of_testing_games)\n",
    "    g.display=False\n",
    "    result=g.run(Q1_agent,Q2_agent)\n",
    "    \n",
    "    total_number_of_games+=number_training_games\n",
    "    win_percentage=sum([r==1 for r in result])/number_training_games*100\n",
    "    loss_percentage=sum([r==2 for r in result])/number_training_games*100\n",
    "    tie_percentage=sum([r==0 for r in result])/number_training_games*100\n",
    "\n",
    "    print(total_number_of_games,\":\",win_percentage,\" \",end=\"\")\n",
    "    \n",
    "    SaveTable(Q1_agent.Q,'Q1_data.json')\n",
    "    SaveTable(Q2_agent.Q,'Q2_data.json')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
