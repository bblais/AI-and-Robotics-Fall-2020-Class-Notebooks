{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  0.2.23\n"
     ]
    }
   ],
   "source": [
    "from Game import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nim\n",
    "\n",
    "Rules:\n",
    "\n",
    "1. start with 21 sticks -- state = number of sticks\n",
    "2. turns alternate taking 1,2, or 3 sticks\n",
    "3. player taking last stick loses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state():\n",
    "    return randint(10,24)\n",
    "\n",
    "def show_state(state):\n",
    "    print(\"There are \",state,\"sticks.\")\n",
    "    \n",
    "def valid_moves(state,player):\n",
    "    if state==1:\n",
    "        return [1]\n",
    "    elif state==2:\n",
    "        return [1,2]\n",
    "    else:\n",
    "        return [1,2,3]\n",
    "    \n",
    "def update_state(state,player,move):\n",
    "    # move = number of sticks to pick up\n",
    "    new_state=state-move # remove the sticks\n",
    "    return new_state\n",
    "\n",
    "def win_status(state,player):\n",
    "    if state==0:\n",
    "        return 'lose'\n",
    "    elif state==1:\n",
    "        return 'win'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # there is no stalemate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_move(state,player):\n",
    "    print(\"Player \",player)\n",
    "    move=int(input(\"How many sticks?\"))\n",
    "    return move\n",
    "\n",
    "human_agent=Agent(human_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_move(state,player):\n",
    "    possible_moves=valid_moves(state,player)\n",
    "    move=random.choice(possible_moves)\n",
    "    return move\n",
    "random_agent=Agent(random_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfect_move(state,player):\n",
    "    move=(state-1)%4\n",
    "    if move==0:\n",
    "        move=1\n",
    "    return move\n",
    "perfect_agent=Agent(perfect_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Game.minimax import *\n",
    "def minimax_move(state,player):\n",
    "\n",
    "    values,moves=minimax_values(state,player,display=False)\n",
    "    return top_choice(moves,values)\n",
    "\n",
    "\n",
    "minimax_agent=Agent(minimax_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skittles_move(state,player,info):\n",
    "    S=info.S\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "    \n",
    "    \n",
    "    # if Ive never seen this state before\n",
    "    if not state in S:\n",
    "        actions=valid_moves(state,player)\n",
    "\n",
    "        S[state]=Table()\n",
    "        for action in actions:\n",
    "            S[state][action]=3     \n",
    "    \n",
    "    move=weighted_choice(S[state])  # weighted across actions\n",
    "    \n",
    "    # what if there are no skittles for a particular state?\n",
    "    # move is None in that case\n",
    "    \n",
    "    if move is None:\n",
    "        # learn a little bit\n",
    "        if last_state:\n",
    "            S[last_state][last_action]=S[last_state][last_action]-1\n",
    "            if S[last_state][last_action]<0:\n",
    "                S[last_state][last_action]=0\n",
    "        \n",
    "        move=random_move(state,player)\n",
    "    \n",
    "    return move\n",
    "\n",
    "def skittles_after(status,player,info):\n",
    "    S=info.S\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "\n",
    "    if status=='lose':\n",
    "        # learn a little bit\n",
    "        S[last_state][last_action]=S[last_state][last_action]-1\n",
    "        if S[last_state][last_action]<0:\n",
    "            S[last_state][last_action]=0\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "skittles_agent=Agent(skittles_move)\n",
    "skittles_agent.S=Table()\n",
    "skittles_agent.post=skittles_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_move(state,player,info):\n",
    "    Q=info.Q\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "    \n",
    "    α=info.α\n",
    "    γ=info.γ\n",
    "    ϵ=info.ϵ\n",
    "    \n",
    "\n",
    "    # if Ive never seen this state before\n",
    "    if not state in Q:\n",
    "        actions=valid_moves(state,player)\n",
    "\n",
    "        Q[state]=Table()\n",
    "        for action in actions:\n",
    "            Q[state][action]=0     \n",
    "    \n",
    "    # deal with random vs top choice here\n",
    "    if random.random()<ϵ:\n",
    "        move=random_move(state,player)  \n",
    "    else:\n",
    "        move=top_choice(Q[state]) \n",
    "    \n",
    "    # what if there are no skittles for a particular state?\n",
    "    # move is None in that case\n",
    "    \n",
    "    if not last_action is None:  # not the first move\n",
    "        # learn a little bit\n",
    "        # change equation here\n",
    "        reward=0\n",
    "        \n",
    "        # Bellman equation\n",
    "        Q[last_state][last_action] += α*(reward+\n",
    "                         γ*max([Q[state][a] for a in Q[state]])  - \n",
    "                                Q[last_state][last_action])\n",
    "    \n",
    "        \n",
    "    \n",
    "    return move\n",
    "\n",
    "def Q_after(status,player,info):\n",
    "    Q=info.Q\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "\n",
    "    α=info.α\n",
    "    γ=info.γ\n",
    "    ϵ=info.ϵ\n",
    "    \n",
    "    if status=='lose':\n",
    "        reward=-1\n",
    "    elif status=='win':\n",
    "        reward=1\n",
    "    elif status=='stalemate':\n",
    "        reward=0.5\n",
    "    else:\n",
    "        reward=0\n",
    "        \n",
    "    # learn a little bit\n",
    "    Q[last_state][last_action] += α*(reward-Q[last_state][last_action])\n",
    "        \n",
    "\n",
    "\n",
    "Q_agent=Agent(Q_move)\n",
    "Q_agent.Q=LoadTable('Q_data.json')\n",
    "Q_agent.post=Q_after\n",
    "\n",
    "Q_agent.α=0.3  # learning rate\n",
    "Q_agent.γ=0.9  # memory constant, discount factor\n",
    "Q_agent.ϵ=0.1  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training stage\n",
    "\n",
    "- the Q values can change\n",
    "- that the agent takes random moves sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_agent.α=0.3  # learning rate\n",
    "Q_agent.ϵ=0.1  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=Game(100)\n",
    "g.display=False\n",
    "g.run(perfect_agent,Q_agent);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing stage\n",
    "\n",
    "- the Q values **do not change**\n",
    "- that the agent **never takes random moves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_agent.α=0.0  # learning rate\n",
    "Q_agent.ϵ=0.0  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=Game(10)\n",
    "g.display=False\n",
    "g.run(perfect_agent,Q_agent);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of games:  10\n",
      "Winning 100.00 percent\n",
      "Losing 0.00 percent\n",
      "Tie 0.00 percent\n"
     ]
    }
   ],
   "source": [
    "g.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {1: 0.9999922690062802, 2: -0.657},\n",
       " 3: {1: -0.3, 2: 0.9999999632966318, 3: -0.3},\n",
       " 4: {1: -0.882351, 2: -0.882351, 3: 0.9999998471329936},\n",
       " 5: {1: -0.9999999820153496, 2: -0.9999999820153496, 3: -0.9999999874107447},\n",
       " 6: {1: 0.8998871819717115, 2: -0.657, 3: -0.3},\n",
       " 7: {1: 0, 2: 0.8999574131741293, 3: -0.657},\n",
       " 8: {1: -0.8234429353958435, 2: 0, 3: 0.8999993771091555},\n",
       " 9: {1: -0.8999994062712278, 2: -0.8999993372089824, 3: -0.8999994919396499},\n",
       " 10: {1: 0.8098502078973727, 2: -0.57459723852663, 3: -0.43613623661223455},\n",
       " 11: {1: -0.31808257341480006, 2: 0.8098515993077074, 3: 0},\n",
       " 12: {1: -0.5325680449554707, 2: -0.21739672590588902, 3: 0.8099979804135733},\n",
       " 13: {1: -0.809930755880558, 2: -0.8099405930946288, 3: -0.8099457092923006},\n",
       " 14: {1: 0.7284028814967268, 2: -0.5170723835347195, 3: -0.4522928158670315},\n",
       " 15: {1: -0.20821947942937508, 2: 0.7287188645048186, 3: -0.3809729052891},\n",
       " 16: {1: -0.39075822593747767, 2: -0.10533437801910903, 3: 0.7289580891570612},\n",
       " 17: {1: -0.7257072567456402, 2: -0.7249919735625803, 3: -0.7252968046932186},\n",
       " 18: {1: 0.6551058991978964, 2: -0.5238516245907714, 3: -0.035665596},\n",
       " 19: {1: -0.07306850481125086, 2: 0.6543479904829171, 3: -0.17656477338191176},\n",
       " 20: {1: -0.02755878181911, 2: -0.09137518828627986, 3: 0.6555045515099381},\n",
       " 21: {1: -0.6057275254929602, 2: -0.6134652988947943, 3: -0.6171413813148945}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveTable(Q_agent.Q,'Q_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q vs Q with progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_agent=Agent(Q_move)\n",
    "Q1_agent.Q=LoadTable('Q1_data.json')\n",
    "Q1_agent.post=Q_after\n",
    "\n",
    "Q1_agent.α=0.3  # learning rate\n",
    "Q1_agent.γ=0.9  # memory constant, discount factor\n",
    "Q1_agent.ϵ=0.1  # probability of a random move during learning\n",
    "\n",
    "Q2_agent=Agent(Q_move)\n",
    "Q2_agent.Q=LoadTable('Q2_data.json')\n",
    "Q2_agent.post=Q_after\n",
    "\n",
    "Q2_agent.α=0.3  # learning rate\n",
    "Q2_agent.γ=0.9  # memory constant, discount factor\n",
    "Q2_agent.ϵ=0.1  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 : 60.0  20 : 60.0  30 : 70.0  40 : 70.0  50 : 80.0  60 : 70.0  70 : 90.0  80 : 60.0  90 : 100.0  100 : 80.0  110 : 90.0  120 : 90.0  130 : 90.0  140 : 80.0  150 : 50.0  160 : 80.0  170 : 70.0  180 : 70.0  190 : 80.0  200 : 90.0  210 : 100.0  220 : 90.0  230 : 80.0  240 : 80.0  250 : 80.0  260 : 70.0  270 : 60.0  280 : 80.0  290 : 80.0  300 : 80.0  310 : 80.0  320 : 70.0  330 : 80.0  340 : 70.0  350 : 80.0  360 : 80.0  370 : 60.0  380 : 60.0  390 : 70.0  400 : 70.0  410 : 80.0  420 : 90.0  430 : 80.0  440 : 90.0  450 : 70.0  460 : 80.0  470 : 90.0  480 : 80.0  490 : 70.0  500 : 70.0  510 : 50.0  520 : 80.0  530 : 90.0  540 : 100.0  550 : 80.0  560 : 90.0  570 : 90.0  580 : 70.0  590 : 70.0  600 : 80.0  610 : 80.0  620 : 70.0  630 : 80.0  640 : 80.0  650 : 80.0  660 : 100.0  670 : 90.0  680 : 80.0  690 : 90.0  700 : 80.0  710 : 100.0  720 : 90.0  730 : 90.0  740 : 90.0  750 : 90.0  760 : 60.0  770 : 60.0  780 : 80.0  790 : 60.0  800 : 80.0  810 : 100.0  820 : 100.0  830 : 80.0  840 : 70.0  850 : 70.0  860 : 80.0  870 : 100.0  880 : 80.0  890 : 70.0  900 : 80.0  910 : 80.0  920 : 40.0  930 : 80.0  940 : 80.0  950 : 60.0  960 : 80.0  970 : 80.0  980 : 80.0  990 : 60.0  1000 : 60.0  "
     ]
    }
   ],
   "source": [
    "total_number_of_games=0\n",
    "for epoch in range(100):\n",
    "    \n",
    "    number_training_games=10\n",
    "    number_of_testing_games=10\n",
    "    \n",
    "    #=================\n",
    "    # traning cycle\n",
    "    Q1_agent.α=0.3  # learning rate\n",
    "    Q1_agent.ϵ=0.1  # probability of a random move during learning\n",
    "    Q2_agent.α=0.3  # learning rate\n",
    "    Q2_agent.ϵ=0.1  # probability of a random move during learning\n",
    "    \n",
    "    g=Game(number_training_games)\n",
    "    g.display=False\n",
    "    g.run(Q1_agent,Q2_agent)\n",
    "\n",
    "    #=================\n",
    "    # testing cycle\n",
    "    Q1_agent.α=0.0  # learning rate\n",
    "    Q1_agent.ϵ=0.0  # probability of a random move during learning\n",
    "    Q2_agent.α=0.0  # learning rate\n",
    "    Q2_agent.ϵ=0.0  # probability of a random move during learning\n",
    "    \n",
    "    \n",
    "    g=Game(number_of_testing_games)\n",
    "    g.display=False\n",
    "    result=g.run(Q1_agent,Q2_agent)\n",
    "    \n",
    "    total_number_of_games+=number_training_games\n",
    "    win_percentage=sum([r==1 for r in result])/number_training_games*100\n",
    "    loss_percentage=sum([r==2 for r in result])/number_training_games*100\n",
    "    tie_percentage=sum([r==0 for r in result])/number_training_games*100\n",
    "\n",
    "    print(total_number_of_games,\":\",win_percentage,\" \",end=\"\")\n",
    "    \n",
    "    SaveTable(Q1_agent.Q,'Q1_data.json')\n",
    "    SaveTable(Q2_agent.Q,'Q2_data.json')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA_move(state,player,info):\n",
    "    Q=info.Q\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "    \n",
    "    α=info.α\n",
    "    γ=info.γ\n",
    "    ϵ=info.ϵ\n",
    "    \n",
    "\n",
    "    # if Ive never seen this state before\n",
    "    if not state in Q:\n",
    "        actions=valid_moves(state,player)\n",
    "\n",
    "        Q[state]=Table()\n",
    "        for action in actions:\n",
    "            Q[state][action]=0     \n",
    "    \n",
    "    # deal with random vs top choice here\n",
    "    if random.random()<ϵ:\n",
    "        move=random_move(state,player)  \n",
    "    else:\n",
    "        move=top_choice(Q[state]) \n",
    "    \n",
    "    # what if there are no skittles for a particular state?\n",
    "    # move is None in that case\n",
    "    \n",
    "    if not last_action is None:  # not the first move\n",
    "        # learn a little bit\n",
    "        # change equation here\n",
    "        reward=0\n",
    "        \n",
    "        # Bellman equation\n",
    "        Q[last_state][last_action] += α*(reward+\n",
    "                         γ*Q[state][move] - \n",
    "                                Q[last_state][last_action])\n",
    "    \n",
    "        \n",
    "    \n",
    "    return move\n",
    "\n",
    "def SARSA_after(status,player,info):\n",
    "    Q=info.Q\n",
    "    last_action=info.last_action\n",
    "    last_state=info.last_state\n",
    "\n",
    "    α=info.α\n",
    "    γ=info.γ\n",
    "    ϵ=info.ϵ\n",
    "    \n",
    "    if status=='lose':\n",
    "        reward=-1\n",
    "    elif status=='win':\n",
    "        reward=1\n",
    "    elif status=='stalemate':\n",
    "        reward=0.5\n",
    "    else:\n",
    "        reward=0\n",
    "        \n",
    "    # learn a little bit\n",
    "    Q[last_state][last_action] += α*(reward-Q[last_state][last_action])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARSA1_agent=Agent(SARSA_move)\n",
    "SARSA1_agent.Q=LoadTable('SARSA1_data.json')\n",
    "SARSA1_agent.post=Q_after\n",
    "\n",
    "SARSA1_agent.α=0.3  # learning rate\n",
    "SARSA1_agent.γ=0.9  # memory constant, discount factor\n",
    "SARSA1_agent.ϵ=0.1  # probability of a random move during learning\n",
    "\n",
    "SARSA2_agent=Agent(Q_move)\n",
    "SARSA2_agent.Q=LoadTable('SARSA2_data.json')\n",
    "SARSA2_agent.post=Q_after\n",
    "\n",
    "SARSA2_agent.α=0.3  # learning rate\n",
    "SARSA2_agent.γ=0.9  # memory constant, discount factor\n",
    "SARSA2_agent.ϵ=0.1  # probability of a random move during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 : 80.0  20 : 30.0  30 : 30.0  40 : 100.0  50 : 20.0  60 : 70.0  70 : 40.0  80 : 20.0  90 : 40.0  100 : 50.0  110 : 80.0  120 : 90.0  130 : 90.0  140 : 70.0  150 : 90.0  160 : 80.0  170 : 90.0  180 : 100.0  190 : 80.0  200 : 70.0  210 : 90.0  220 : 80.0  230 : 60.0  240 : 80.0  250 : 70.0  260 : 100.0  270 : 100.0  280 : 70.0  290 : 90.0  300 : 90.0  310 : 100.0  320 : 70.0  330 : 70.0  340 : 90.0  350 : 70.0  360 : 70.0  370 : 100.0  380 : 100.0  390 : 80.0  400 : 80.0  410 : 80.0  420 : 100.0  430 : 80.0  440 : 90.0  450 : 70.0  460 : 70.0  470 : 70.0  480 : 80.0  490 : 70.0  500 : 80.0  510 : 90.0  520 : 70.0  530 : 100.0  540 : 40.0  550 : 90.0  560 : 80.0  570 : 80.0  580 : 70.0  590 : 100.0  600 : 70.0  610 : 70.0  620 : 90.0  630 : 80.0  640 : 90.0  650 : 60.0  660 : 70.0  670 : 70.0  680 : 90.0  690 : 80.0  700 : 70.0  710 : 60.0  720 : 90.0  730 : 80.0  740 : 90.0  750 : 70.0  760 : 90.0  770 : 90.0  780 : 100.0  790 : 90.0  800 : 90.0  810 : 80.0  820 : 100.0  830 : 80.0  840 : 100.0  850 : 100.0  860 : 70.0  870 : 90.0  880 : 90.0  890 : 80.0  900 : 100.0  910 : 80.0  920 : 70.0  930 : 60.0  940 : 90.0  950 : 70.0  960 : 80.0  970 : 90.0  980 : 90.0  990 : 80.0  1000 : 80.0  "
     ]
    }
   ],
   "source": [
    "total_number_of_games=0\n",
    "for epoch in range(100):\n",
    "    \n",
    "    number_training_games=10\n",
    "    number_of_testing_games=10\n",
    "    \n",
    "    #=================\n",
    "    # traning cycle\n",
    "    SARSA1_agent.α=0.3  # learning rate\n",
    "    SARSA1_agent.ϵ=0.1  # probability of a random move during learning\n",
    "    SARSA2_agent.α=0.3  # learning rate\n",
    "    SARSA2_agent.ϵ=0.1  # probability of a random move during learning\n",
    "    \n",
    "    g=Game(number_training_games)\n",
    "    g.display=False\n",
    "    g.run(SARSA1_agent,SARSA2_agent)\n",
    "\n",
    "    #=================\n",
    "    # testing cycle\n",
    "    SARSA1_agent.α=0.0  # learning rate\n",
    "    SARSA1_agent.ϵ=0.0  # probability of a random move during learning\n",
    "    SARSA2_agent.α=0.0  # learning rate\n",
    "    SARSA2_agent.ϵ=0.0  # probability of a random move during learning\n",
    "    \n",
    "    \n",
    "    g=Game(number_of_testing_games)\n",
    "    g.display=False\n",
    "    result=g.run(SARSA1_agent,SARSA2_agent)\n",
    "    \n",
    "    total_number_of_games+=number_training_games\n",
    "    win_percentage=sum([r==1 for r in result])/number_training_games*100\n",
    "    loss_percentage=sum([r==2 for r in result])/number_training_games*100\n",
    "    tie_percentage=sum([r==0 for r in result])/number_training_games*100\n",
    "\n",
    "    print(total_number_of_games,\":\",win_percentage,\" \",end=\"\")\n",
    "    \n",
    "    SaveTable(SARSA1_agent.Q,'SARSA1_data.json')\n",
    "    SaveTable(SARSA2_agent.Q,'SARSA2_data.json')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23: {1: -0.10148915487549862, 2: 0.4319029342815772, 3: -0.1180982401597065},\n",
       " 20: {1: -0.238160665548873, 2: -0.24436750301464205, 3: 0.583987162696529},\n",
       " 17: {1: -0.5501124307406864, 2: -0.462662241080866, 3: -0.6606470226736118},\n",
       " 14: {1: 0.656639755013361, 2: -0.6655696062504155, 3: -0.7020860300979508},\n",
       " 11: {1: -0.5253270520467264, 2: 0.8089325268398339, 3: -0.16048599548526926},\n",
       " 9: {1: -0.8058164626637306, 2: -0.7926785201316862, 3: -0.8052623045281572},\n",
       " 4: {1: -0.959646393, 2: -0.7599, 3: 0.9999999999999999},\n",
       " 18: {1: 0.5975635737494217, 2: -0.5160238239821485, 3: -0.4341635361541453},\n",
       " 6: {1: 0.3616290822189513, 2: -0.5274768316076847, 3: -0.9176457},\n",
       " 2: {1: 0.9999999999999993, 2: -0.959646393},\n",
       " 10: {1: 0.8090211723353758, 2: -0.46261493419267696, 3: -0.10582126971620232},\n",
       " 12: {1: -0.5297565953023181, 2: -0.3345145825302327, 3: 0.36741337039089655},\n",
       " 3: {1: -0.9956797190259519, 2: 0.9999999999999999, 3: -0.9903110989592999},\n",
       " 19: {1: -0.2765340431867735, 2: 0.5746964845270316, 3: -0.47038329352084385},\n",
       " 15: {1: -0.2992190045065088, 2: 0.6712642119216757, 3: -0.33707823581314394},\n",
       " 8: {1: -0.8267609296672644, 2: -0.7557044476896532, 3: 0.8999707331821181},\n",
       " 13: {1: -0.5912181393934061, 2: -0.7070260991478295, 3: -0.7547170722007221},\n",
       " 24: {1: 0.12357926681616539, 2: 0.005794964238703216, 3: 0.10828975046315079},\n",
       " 16: {1: -0.34598622455721284, 2: -0.43574098464333155, 3: 0.6753015201819099},\n",
       " 7: {1: -0.7402985270085658, 2: 0.8999999430886869, 3: -0.46401000000000003},\n",
       " 21: {1: -0.4861644613141558,\n",
       "  2: -0.47790924743774166,\n",
       "  3: -0.41592380728932987},\n",
       " 5: {1: -0.9865439961065771, 2: -0.9999999576803777, 3: -0.9999999250951669},\n",
       " 22: {1: 0.4356012293616338, 2: -0.03411078336954614, 3: -0.15788743963857557}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SARSA1_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
